# üîç Reporte T√©cnico: Workers ‚Üî GCS Connectivity Diagnostic

**Fecha:** 2026-02-09  
**Autor:** Cloud Infrastructure Engineer & Backend Developer  
**Versi√≥n:** 1.0

---

## Resumen Ejecutivo

Se audit√≥ la conectividad entre los Celery Workers locales y Google Cloud Storage (GCS). Se encontraron **7 puntos de falla** que impiden la comunicaci√≥n. El m√°s cr√≠tico: **`STORAGE_BACKEND=local`** en el `.env` ra√≠z, forzando a todos los workers a escribir al filesystem local en vez de GCS.

### Archivos Auditados

| Archivo | Prop√≥sito |
|---------|-----------|
| `celery_app.py` (root) | Config Celery (4 tasks) |
| `workers/celery_app.py` | Config Celery (8 tasks) ‚Äî usado por Docker |
| `app/services/gcs_service.py` | Cliente GCS con ADC (Singleton) |
| `app/services/storage_service.py` | Storage multi-backend: gcs/r2/local |
| `app/workers/exploration_hd_worker.py` | Worker HD images ‚Üí StorageService |
| `app/services/closure_report_service.py` | Closure reports ‚Üí StorageService |
| `app/services/imagery_service.py` | Carousel thumbnails ‚Üí StorageService |
| `workers/tasks/ingestion.py` | Ingesta FIRMS (sin GCS) |
| `workers/tasks/carousel_task.py` | Task Celery ‚Üí ImageryService |
| `workers/tasks/closure_report_task.py` | Task Celery ‚Üí ClosureReportService |
| `docker-compose.yml` | Orquestaci√≥n de servicios |
| `Dockerfile.worker` | Imagen Docker para workers |
| `.env` (root) | Variables de entorno desarrollo |
| `docker/.env` | Variables de entorno Docker |

---

## 1. Arquitectura de Storage Actual

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    WORKER ‚Üí STORAGE FLOW                            ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                      ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îÇ
‚îÇ  ‚îÇ worker-     ‚îÇ   ‚îÇ worker-     ‚îÇ   ‚îÇ worker-     ‚îÇ              ‚îÇ
‚îÇ  ‚îÇ ingestion   ‚îÇ   ‚îÇ clustering  ‚îÇ   ‚îÇ analysis    ‚îÇ               ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò               ‚îÇ
‚îÇ         ‚îÇ                  ‚îÇ                 ‚îÇ                      ‚îÇ
‚îÇ         ‚îÇ (no GCS)         ‚îÇ (no GCS)        ‚îÇ                      ‚îÇ
‚îÇ         ‚îÇ                  ‚îÇ                 ‚îú‚îÄ‚îÄ carousel_task      ‚îÇ
‚îÇ         ‚ñº                  ‚ñº                 ‚îú‚îÄ‚îÄ closure_report     ‚îÇ
‚îÇ    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê             ‚îú‚îÄ‚îÄ exploration_hd     ‚îÇ
‚îÇ    ‚îÇ  DB     ‚îÇ       ‚îÇ  DB     ‚îÇ             ‚îÇ                      ‚îÇ
‚îÇ    ‚îÇ  only   ‚îÇ       ‚îÇ  only   ‚îÇ             ‚ñº                      ‚îÇ
‚îÇ    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê             ‚îÇ
‚îÇ                                        ‚îÇ Storage     ‚îÇ             ‚îÇ
‚îÇ                                        ‚îÇ Service     ‚îÇ             ‚îÇ
‚îÇ                                        ‚îÇ (backend?)  ‚îÇ             ‚îÇ
‚îÇ                                        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò             ‚îÇ
‚îÇ                                ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ
‚îÇ                                ‚ñº              ‚ñº              ‚ñº     ‚îÇ
‚îÇ                         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ                         ‚îÇ local ‚ö†Ô∏è ‚îÇ  ‚îÇ gcs ‚úÖ   ‚îÇ  ‚îÇ r2       ‚îÇ  ‚îÇ
‚îÇ                         ‚îÇ./storage/‚îÇ  ‚îÇ Buckets  ‚îÇ  ‚îÇ (legacy) ‚îÇ  ‚îÇ
‚îÇ                         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                              ‚Üë                                     ‚îÇ
‚îÇ                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                     ‚îÇ
‚îÇ                    ‚îÇ ACTUALMENTE ACTIVO                             ‚îÇ
‚îÇ                    ‚îÇ STORAGE_BACKEND=local                          ‚îÇ
‚îÇ                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                             ‚îÇ
‚îÇ                                                                      ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                                   ‚îÇ
‚îÇ  ‚îÇ GCSService  ‚îÇ ‚Üê Singleton, NO usado por workers                 ‚îÇ
‚îÇ  ‚îÇ (legacy)    ‚îÇ   Crashea en import si falta GCS_PROJECT_ID       ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                                   ‚îÇ
‚îÇ                                                                      ‚îÇ
‚îÇ  GCS BUCKETS TARGET:                                                ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îÇ
‚îÇ  ‚îÇ forestguard-images  ‚îÇ  ‚îÇ forestguard-     ‚îÇ  ‚îÇ forestguard-    ‚îÇ‚îÇ
‚îÇ  ‚îÇ (thumbnails, HD,    ‚îÇ  ‚îÇ reports          ‚îÇ  ‚îÇ certificates    ‚îÇ‚îÇ
‚îÇ  ‚îÇ  capas satelitales) ‚îÇ  ‚îÇ (cierre, evid.)  ‚îÇ  ‚îÇ (auditor√≠a)     ‚îÇ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÇ
‚îÇ                                                                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

> **IMPORTANTE:** Los workers usan `StorageService` (no `GCSService`). `StorageService` soporta 3 backends: `gcs`, `r2`, `local`. Actualmente el `.env` lo configura como `local`.

---

## 2. Puntos de Falla Encontrados

### ‚ùå PF-1: `STORAGE_BACKEND=local` en `.env` ra√≠z (CR√çTICO)

| Campo | Detalle |
|-------|---------|
| **Archivo** | `.env` l√≠nea 108 |
| **Contenido** | `STORAGE_BACKEND=local #gcs` |
| **Impacto** | **Todos los workers escriben al filesystem local `./storage/`, NO a GCS** |
| **Fix** | Cambiar a `STORAGE_BACKEND=gcs` |

```diff
-STORAGE_BACKEND=local #gcs
+STORAGE_BACKEND=gcs
```

---

### ‚ùå PF-2: `GOOGLE_APPLICATION_CREDENTIALS` comentado en `.env`

| Campo | Detalle |
|-------|---------|
| **Archivo** | `.env` l√≠nea 112 |
| **Contenido** | `#GOOGLE_APPLICATION_CREDENTIALS="C:\ruta\absoluta\service-account-gcs.json"` |
| **Impacto** | Sin credenciales expl√≠citas, `StorageService` falla al crear el cliente GCS |
| **Fix** | Descomentar y apuntar a la ruta del SA JSON, o usar ADC (`gcloud auth application-default login`) |

```diff
-#GOOGLE_APPLICATION_CREDENTIALS="C:\ruta\absoluta\service-account-gcs.json"
+GOOGLE_APPLICATION_CREDENTIALS=C:\Users\nicog\.config\gcloud\application_default_credentials.json
```

**Alternativa recomendada:** Ejecutar `gcloud auth application-default login` una vez y el SDK detecta credenciales autom√°ticamente.

---

### ‚ùå PF-3: `gcs_service.py` Singleton crash at import

| Campo | Detalle |
|-------|---------|
| **Archivo** | `app/services/gcs_service.py` l√≠nea 395 |
| **Contenido** | `gcs_service = GCSService()` |
| **Impacto** | Cualquier `import` de este m√≥dulo crashea si `GCS_PROJECT_ID` no est√° definido ‚Üí `ValueError` inmediato |

> **NOTA:** Este servicio NO es usado por los workers (usan `StorageService`), pero cualquier import transitivo puede causar crash en toda la aplicaci√≥n.

```diff
 # Singleton global
-gcs_service = GCSService()
+# Lazy initialization to avoid crashes on import
+gcs_service = None
+
+def get_gcs_service() -> GCSService:
+    global gcs_service
+    if gcs_service is None:
+        gcs_service = GCSService()
+    return gcs_service
```

---

### ‚ö†Ô∏è PF-4: Dos `celery_app.py` con listas de tasks divergentes

| Campo | Detalle |
|-------|---------|
| **Archivos** | `celery_app.py` (root) vs `workers/celery_app.py` |
| **Diferencia** | Root tiene **4 tasks**; `workers/` tiene **8 tasks** (incluye `carousel_task`, `closure_report_task`, `episode_merge_task`, `clustering_task`) |
| **Impacto** | Docker usa `workers.celery_app` (correcto), pero desarrollo local puede apuntar al root |
| **Fix** | Eliminar `celery_app.py` del root o sincronizar |

---

### ‚ö†Ô∏è PF-5: Docker user home path mismatch

| Campo | Detalle |
|-------|---------|
| **Archivo** | `docker-compose.yml` l√≠nea 53, 88, 127, 160 |
| **Contenido** | Monta en `/home/user/.config/gcloud/` |
| **Problema** | `Dockerfile.worker` crea usuario **`celery`** (no `user`). Path correcto: `/home/celery/.config/gcloud/` |

```diff
 environment:
-  GOOGLE_APPLICATION_CREDENTIALS: /home/user/.config/gcloud/application_default_credentials.json
+  GOOGLE_APPLICATION_CREDENTIALS: /home/celery/.config/gcloud/application_default_credentials.json
 volumes:
-  - ~/.config/gcloud:/home/user/.config/gcloud:ro
+  - ~/.config/gcloud:/home/celery/.config/gcloud:ro
```

---

### ‚ö†Ô∏è PF-6: Workers faltantes de `STORAGE_BUCKET_*` en docker-compose

| Campo | Detalle |
|-------|---------|
| **Archivo** | `docker-compose.yml` l√≠neas 74-179 |
| **Problema** | Solo el servicio `api` tiene `STORAGE_BUCKET_IMAGES/REPORTS/CERTIFICATES`. Los workers `worker-ingestion`, `worker-clustering`, y `worker-analysis` NO los tienen |
| **Impacto** | Workers usan los valores hardcodeados del c√≥digo (que coinciden, pero no es configurable) |
| **Fix** | Agregar las 3 variables `STORAGE_BUCKET_*` a todos los workers |

---

### ‚ö†Ô∏è PF-7: `worker_prefetch_multiplier` duplicado en `celery_app.py` ra√≠z

| Campo | Detalle |
|-------|---------|
| **Archivo** | `celery_app.py` (root) l√≠neas 50 y 63 |
| **Impacto** | Menor, pero indica c√≥digo no mantenido/duplicado |

---

## 3. Validaci√≥n del Flujo de Datos

### 3.1 Workers que Usan GCS (v√≠a `StorageService`)

| Worker Task | Service Intermediario | Bucket Target | Operaci√≥n |
|-------------|----------------------|---------------|-----------|
| `carousel_task` | `ImageryService` ‚Üí `StorageService` | `forestguard-images` | Upload thumbnails para carrusel |
| `closure_report_task` | `ClosureReportService` ‚Üí `StorageService` | `forestguard-images` | Upload im√°genes pre/post incendio |
| `exploration_hd_worker` | `StorageService` directamente | `forestguard-images` | Upload im√°genes HD de exploraci√≥n |

### 3.2 Workers que NO Usan GCS (actualmente)

| Worker Task | Service | Almacenamiento |
|-------------|---------|----------------|
| `ingestion` (download_firms_daily) | `load_firms_incremental` | Inserci√≥n directa en DB (sin archivos) |
| `clustering` | `DetectionClusteringService` | Solo DB |
| `episode_merge_task` | `EpisodeService` | Solo DB |

### 3.3 Permisos IAM Necesarios (Scopes)

La Service Account de GCS necesita estos roles en cada bucket:

| Role | Bucket | Raz√≥n |
|------|--------|-------|
| `roles/storage.objectAdmin` | `forestguard-images` | Upload + delete thumbnails, HD, capas satelitales |
| `roles/storage.objectCreator` | `forestguard-reports` | Upload PDFs de reportes de cierre y evidencia |
| `roles/storage.objectCreator` | `forestguard-certificates` | Upload certificados de auditor√≠a y recibos |

> **Para configurar desde GCP Console:**
> ```bash
> # Otorgar permisos a la Service Account
> SA_EMAIL="gcs-sa@project-fd452487-efa4-4858-8a7.iam.gserviceaccount.com"
> 
> gsutil iam ch serviceAccount:$SA_EMAIL:objectAdmin gs://forestguard-images
> gsutil iam ch serviceAccount:$SA_EMAIL:objectCreator gs://forestguard-reports
> gsutil iam ch serviceAccount:$SA_EMAIL:objectCreator gs://forestguard-certificates
> ```

---

## 4. H3 y GCS (Validaci√≥n T1.3)

Seg√∫n la arquitectura, ciertos datos H3 procesados por workers deber√≠an guardarse como archivos **Parquet en GCS** para ahorrar espacio en Supabase.

**Estado actual:**

| Aspecto | Estado | Detalle |
|---------|--------|---------|
| C√≥digo de exportaci√≥n Parquet | ‚ùå No implementado | No existe worker de exportaci√≥n |
| `h3_recurrence_stats` | ‚úÖ En PostgreSQL | Vista materializada, no archivos Parquet |
| `fire_events.h3_index` | ‚úÖ En PostgreSQL | Columna `BIGINT` con √≠ndice H3 |
| C√°lculo de recurrencia | ‚úÖ En BD | Precalculado via vista materializada |

> **Conclusi√≥n:** La estrategia Parquet-en-GCS para H3 a√∫n **no est√° implementada**. Actualmente todos los datos viven en Supabase PostgreSQL. Esto es una optimizaci√≥n post-MVP que requiere un worker dedicado de exportaci√≥n peri√≥dica.

---

## 5. Configuraci√≥n de Producci√≥n Docker Compose (Propuesta)

```yaml
# ‚îÄ‚îÄ Bloque reutilizable para todos los workers ‚îÄ‚îÄ
x-worker-env: &worker-env
  # Database
  DB_HOST: ${DB_HOST}
  DB_PORT: ${DB_PORT}
  DB_NAME: ${DB_NAME}
  DB_USER: ${DB_USER}
  DB_PASSWORD: ${DB_PASSWORD}
  # GCS (producci√≥n - Service Account JSON montado como secreto Docker)
  GOOGLE_APPLICATION_CREDENTIALS: /run/secrets/gcs-sa-key.json
  GCS_PROJECT_ID: ${GCS_PROJECT_ID}
  STORAGE_BACKEND: gcs
  STORAGE_BUCKET_IMAGES: ${STORAGE_BUCKET_IMAGES:-forestguard-images}
  STORAGE_BUCKET_REPORTS: ${STORAGE_BUCKET_REPORTS:-forestguard-reports}
  STORAGE_BUCKET_CERTIFICATES: ${STORAGE_BUCKET_CERTIFICATES:-forestguard-certificates}
  # Celery
  CELERY_BROKER_URL: redis://redis:6379/0
  CELERY_RESULT_BACKEND: redis://redis:6379/1
  ENVIRONMENT: production

x-worker-config: &worker-config
  build:
    context: .
    dockerfile: Dockerfile.worker
  secrets:
    - gcs-sa-key
  depends_on:
    redis:
      condition: service_healthy
  networks:
    - forestguard
  restart: unless-stopped
  deploy:
    resources:
      limits:
        memory: 512M

secrets:
  gcs-sa-key:
    file: ./secrets/gcs-service-account.json

services:
  worker-ingestion:
    <<: *worker-config
    container_name: forestguard-worker-ingestion
    environment:
      <<: *worker-env
      FIRMS_API_KEY: ${FIRMS_API_KEY}
    command: >
      celery -A workers.celery_app worker
      --loglevel=info --queues=ingestion --concurrency=2

  worker-clustering:
    <<: *worker-config
    container_name: forestguard-worker-clustering
    environment:
      <<: *worker-env
    command: >
      celery -A workers.celery_app worker
      --loglevel=info --queues=clustering --concurrency=2

  worker-analysis:
    <<: *worker-config
    container_name: forestguard-worker-analysis
    environment:
      <<: *worker-env
      GEE_SERVICE_ACCOUNT_JSON: /run/secrets/gcs-sa-key.json
    command: >
      celery -A workers.celery_app worker
      --loglevel=info --queues=analysis --concurrency=1
```

---

## 6. Script de Validaci√≥n

Se gener√≥ `scripts/test_gcs_conn.py` que:

1. Verifica configuraci√≥n de entorno (env vars, credenciales, ADC)
2. Sube un archivo de 1KB a cada uno de los 3 buckets
3. Reporta el error exacto (403 Forbidden, 404 Not Found, 401 Unauthorized, etc.)
4. Verifica lectura y borrado adem√°s de escritura
5. Genera un JSON detallado en `scripts/gcs_diag_report.json`

**Para ejecutar:**
```bash
python scripts/test_gcs_conn.py
```

---

## 7. Resumen de Remediaci√≥n

| # | Acci√≥n | Prioridad | Archivo Afectado |
|---|--------|-----------|------------------|
| 1 | Cambiar `STORAGE_BACKEND=gcs` en `.env` | üî¥ Cr√≠tico | `.env:108` |
| 2 | Descomentar/configurar `GOOGLE_APPLICATION_CREDENTIALS` | üî¥ Cr√≠tico | `.env:112` |
| 3 | Lazy-init de `gcs_service` global | üü° Alto | `app/services/gcs_service.py:395` |
| 4 | Fix user path en docker-compose (`celery` not `user`) | üü° Alto | `docker-compose.yml` |
| 5 | Agregar `STORAGE_BUCKET_*` a todos los workers | üü° Alto | `docker-compose.yml` |
| 6 | Eliminar o sincronizar `celery_app.py` ra√≠z | üü¢ Medio | `celery_app.py` |
| 7 | Ejecutar `test_gcs_conn.py` para validar | üü¢ Medio | `scripts/test_gcs_conn.py` |
| 8 | Implementar exportaci√≥n Parquet H3 a GCS | ‚ö™ Post-MVP | Nuevo worker |

---

*Documento generado: 2026-02-09*  
*Pr√≥ximos pasos: Ver `gcs_remediation_tasks.md` para el plan de tareas t√©cnicas*
